{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keypoint by mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'FPS':30,\n",
    "    'IMG_SIZE':128,\n",
    "    'EPOCHS':30,\n",
    "    'LEARNING_RATE':2e-4,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = r\"D:\\AI_Data\\TV_HMCR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_000</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\train\\TRAIN_000.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_001</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\train\\TRAIN_001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_002</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\train\\TRAIN_002.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                    path  label\n",
       "0  TRAIN_000  D:\\AI_Data\\TV_HMCR\\train\\TRAIN_000.mp4      3\n",
       "1  TRAIN_001  D:\\AI_Data\\TV_HMCR\\train\\TRAIN_001.mp4      0\n",
       "2  TRAIN_002  D:\\AI_Data\\TV_HMCR\\train\\TRAIN_002.mp4      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\test\\TEST_000.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\test\\TEST_001.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\test\\TEST_002.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                  path\n",
       "0  TEST_000  D:\\AI_Data\\TV_HMCR\\test\\TEST_000.mp4\n",
       "1  TEST_001  D:\\AI_Data\\TV_HMCR\\test\\TEST_001.mp4\n",
       "2  TEST_002  D:\\AI_Data\\TV_HMCR\\test\\TEST_002.mp4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((610, 3), (153, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(data_root_path, \"train.csv\"))\n",
    "df_train[\"path\"] = df_train[\"path\"].apply(lambda x: os.path.join(data_root_path, os.path.join(x.split(\"/\")[1], x.split(\"/\")[2])))\n",
    "display(df_train.head(3))\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_root_path, \"test.csv\"))\n",
    "df_test[\"path\"] = df_test[\"path\"].apply(lambda x: os.path.join(data_root_path, os.path.join(x.split(\"/\")[1], x.split(\"/\")[2])))\n",
    "display(df_test.head(3))\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    for _ in range(CFG[\"FPS\"]):\n",
    "        _, img = cap.read()\n",
    "        img = cv2.resize(img, (CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"]))\n",
    "        img = img / 255.\n",
    "        frames.append(img)\n",
    "    return frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((488, 3), (122, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, _, _ = train_test_split(df_train, df_train[\"label\"], test_size=0.2, random_state=CFG[\"SEED\"])\n",
    "\n",
    "train.shape, val.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics.pairwise import itertools\n",
    "\n",
    "class Detecter():\n",
    "    def __init__(self, mode=False, maxHands=2, detection_conf=1, track_conf=0.5):\n",
    "        self.mode = mode\n",
    "        self.maxHands = maxHands\n",
    "        self.detection_conf = detection_conf\n",
    "        self.track_conf = track_conf\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(self.mode, self.maxHands, self.detection_conf, self.track_conf)\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        \n",
    "    def find_hands(self, img, draw=False):\n",
    "        rgb_img = cv2.flip(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), 1)\n",
    "        self.results = self.hands.process(rgb_img)\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for hand_mk, hand_ness in zip(self.results.multi_hand_landmarks, self.results.multi_handedness):\n",
    "                if draw: self.mpDraw.draw_landmarks(img, hand_mk, self.mpHands.HAND_CONNECTIONS)\n",
    "        return img\n",
    "    \n",
    "    def find_position(self, img, hand_no=0, draw=False):\n",
    "        mk = []\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            hand = self.results.multi_hand_landmarks[hand_no]\n",
    "            for _, mk_part in enumerate(hand.landmark):\n",
    "                h, w, c = img.shape\n",
    "                # cx, cy = min(int(mk_part.x*w), w-1), min(int(mk_part.y*h), h-1)\n",
    "                # mk.append([cx, cy])\n",
    "                cx, cy, cz = mk_part.x, mk_part.y, mk_part.z\n",
    "                mk.append(torch.FloatTensor([cx, cy, cz]))\n",
    "                if draw: cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "        return mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m         train_lmk[v] \u001b[39m=\u001b[39m lmk\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m train_lmk\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m---> 18\u001b[0m     train_lmk[v] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(train_lmk[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[1;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "detecter = Detecter()\n",
    "\n",
    "train_lmk = {}\n",
    "\n",
    "for v in range(df_train.shape[0])[:3]:\n",
    "    vPath = df_train.iloc[v][\"path\"]\n",
    "    print(v)\n",
    "    cap = cv2.VideoCapture(vPath)\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if not ret: break\n",
    "        img = detecter.find_hands(img)\n",
    "        lmk = detecter.find_position(img)\n",
    "        \n",
    "        train_lmk[v] = lmk\n",
    "        \n",
    "for v in train_lmk.keys():\n",
    "    train_lmk[v] = torch.stack(train_lmk[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lmk.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6170c7e636c2dbd0f88cb8f557866518ac6c7d83ee4f2709c7df3161954bfcc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
