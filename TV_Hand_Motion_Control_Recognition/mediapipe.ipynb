{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keypoint by mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'FPS':30,\n",
    "    'IMG_SIZE':128,\n",
    "    'EPOCHS':30,\n",
    "    'LEARNING_RATE':2e-4,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = r\"D:\\AI_Data\\TV_HMCR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_000</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\train\\TRAIN_000.mp4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_001</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\train\\TRAIN_001.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_002</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\train\\TRAIN_002.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                    path  label\n",
       "0  TRAIN_000  D:\\AI_Data\\TV_HMCR\\train\\TRAIN_000.mp4      3\n",
       "1  TRAIN_001  D:\\AI_Data\\TV_HMCR\\train\\TRAIN_001.mp4      0\n",
       "2  TRAIN_002  D:\\AI_Data\\TV_HMCR\\train\\TRAIN_002.mp4      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\test\\TEST_000.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\test\\TEST_001.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>D:\\AI_Data\\TV_HMCR\\test\\TEST_002.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                  path\n",
       "0  TEST_000  D:\\AI_Data\\TV_HMCR\\test\\TEST_000.mp4\n",
       "1  TEST_001  D:\\AI_Data\\TV_HMCR\\test\\TEST_001.mp4\n",
       "2  TEST_002  D:\\AI_Data\\TV_HMCR\\test\\TEST_002.mp4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((610, 3), (153, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(data_root_path, \"train.csv\"))\n",
    "df_train[\"path\"] = df_train[\"path\"].apply(lambda x: os.path.join(data_root_path, os.path.join(x.split(\"/\")[1], x.split(\"/\")[2])))\n",
    "display(df_train.head(3))\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(data_root_path, \"test.csv\"))\n",
    "df_test[\"path\"] = df_test[\"path\"].apply(lambda x: os.path.join(data_root_path, os.path.join(x.split(\"/\")[1], x.split(\"/\")[2])))\n",
    "display(df_test.head(3))\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    for _ in range(CFG[\"FPS\"]):\n",
    "        _, img = cap.read()\n",
    "        img = cv2.resize(img, (CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"]))\n",
    "        img = img / 255.\n",
    "        frames.append(img)\n",
    "    return frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((488, 3), (122, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, _, _ = train_test_split(df_train, df_train[\"label\"], test_size=0.2, random_state=CFG[\"SEED\"])\n",
    "\n",
    "train.shape, val.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics.pairwise import itertools\n",
    "\n",
    "class Detecter():\n",
    "    def __init__(self, mode=False, maxHands=2, detection_conf=1, track_conf=0.5):\n",
    "        self.mode = mode\n",
    "        self.maxHands = maxHands\n",
    "        self.detection_conf = detection_conf\n",
    "        self.track_conf = track_conf\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(self.mode, self.maxHands, self.detection_conf, self.track_conf)\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        \n",
    "    def find_hands(self, img, draw=False):\n",
    "        rgb_img = cv2.flip(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), 1)\n",
    "        self.results = self.hands.process(rgb_img)\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for hand_mk, hand_ness in zip(self.results.multi_hand_landmarks, self.results.multi_handedness):\n",
    "                if draw: self.mpDraw.draw_landmarks(img, hand_mk, self.mpHands.HAND_CONNECTIONS)\n",
    "        return img\n",
    "    \n",
    "    def find_position(self, img, hand_no=0, draw=False):\n",
    "        mk = []\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            hand = self.results.multi_hand_landmarks[hand_no]\n",
    "            for _, mk_part in enumerate(hand.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = min(int(mk_part.x*w), w-1), min(int(mk_part.y*h), h-1)\n",
    "                mk.append([cx, cy])\n",
    "                if draw: cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "        return mk\n",
    "    \n",
    "    def preprocess_lmk(self, mk):\n",
    "        temp_lmk_list = deepcopy(mk)\n",
    "        # conver coor\n",
    "        base_x, base_y = 0, 0\n",
    "        for id, lmk in enumerate(temp_lmk_list):\n",
    "            if id==0: base_x, base_y = mk[0], mk[1]\n",
    "            temp_lmk_list[id][0] = temp_lmk_list[id][0] - base_x\n",
    "            temp_lmk_list[id][1] = temp_lmk_list[id][1] - base_y\n",
    "        # convert 1d\n",
    "        temp_lmk_list = list(itertools.chain.from_iterable(temp_lmk_list)) \n",
    "        # Normalize\n",
    "        max_value = max(list(map(abs, temp_lmk_list)), default=0)\n",
    "        return temp_lmk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detecter = Detecter()\n",
    "\n",
    "train_lmk = []\n",
    "\n",
    "for v in range(df_train.shape[0]):\n",
    "    vPath = df_train.iloc[v][\"path\"]\n",
    "    \n",
    "    cap = cv2.VideoCapture(vPath)\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if not ret: break\n",
    "        img = detecter.find_hands(img)\n",
    "        lmk = detecter.find_position(img)\n",
    "        \n",
    "        train_lmk.append(lmk)\n",
    "        print(v, len(lmk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dict = {}\n",
    "train_vpath_key = [f\"v{id}\" for id in range(df_train.shape[0])]\n",
    "lmk_range = [idx for idx in range(0, len(train_lmk), 30)]\n",
    "\n",
    "for k, v in zip(train_vpath_key, lmk_range):\n",
    "    v_dict[k] = train_lmk[v:v+CFG[\"FPS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = pd.DataFrame(v_dict)\n",
    "cor = cor.apply(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[23, 88], [23, 79], [19, 67], [16, 58], [13, ...\n",
       "1     [[23, 90], [24, 79], [22, 68], [19, 60], [17, ...\n",
       "2     [[24, 91], [23, 78], [20, 67], [17, 61], [15, ...\n",
       "3     [[25, 92], [24, 79], [22, 68], [18, 61], [14, ...\n",
       "4     [[26, 92], [24, 79], [22, 66], [18, 59], [15, ...\n",
       "5     [[26, 93], [24, 80], [22, 68], [19, 60], [15, ...\n",
       "6     [[28, 94], [28, 80], [25, 68], [21, 61], [18, ...\n",
       "7     [[29, 94], [28, 80], [26, 67], [22, 60], [19, ...\n",
       "8     [[31, 95], [30, 81], [28, 69], [24, 62], [21, ...\n",
       "9     [[34, 96], [33, 83], [30, 71], [27, 63], [24, ...\n",
       "10    [[36, 96], [36, 84], [33, 72], [30, 64], [28, ...\n",
       "11    [[38, 97], [33, 90], [30, 80], [29, 72], [26, ...\n",
       "12    [[42, 97], [39, 87], [36, 75], [34, 67], [32, ...\n",
       "13    [[44, 98], [40, 88], [37, 75], [36, 66], [36, ...\n",
       "14    [[48, 98], [42, 86], [38, 72], [37, 61], [37, ...\n",
       "15    [[52, 97], [48, 90], [45, 79], [43, 69], [42, ...\n",
       "16    [[55, 96], [51, 92], [48, 82], [46, 75], [44, ...\n",
       "17    [[58, 95], [52, 90], [49, 78], [48, 68], [48, ...\n",
       "18    [[63, 96], [56, 89], [53, 78], [52, 68], [51, ...\n",
       "19    [[69, 97], [63, 87], [59, 75], [57, 66], [55, ...\n",
       "20    [[74, 95], [69, 86], [65, 74], [61, 66], [57, ...\n",
       "21    [[75, 94], [67, 86], [63, 74], [61, 65], [59, ...\n",
       "22    [[79, 96], [68, 86], [65, 73], [63, 65], [62, ...\n",
       "23    [[85, 95], [73, 86], [69, 72], [68, 63], [69, ...\n",
       "24    [[86, 95], [75, 86], [72, 72], [71, 63], [71, ...\n",
       "25    [[90, 95], [78, 83], [75, 70], [75, 61], [73, ...\n",
       "26    [[92, 95], [80, 84], [78, 70], [77, 62], [75, ...\n",
       "27    [[95, 95], [85, 85], [82, 71], [81, 62], [80, ...\n",
       "28    [[99, 94], [87, 84], [84, 70], [83, 62], [83, ...\n",
       "29    [[101, 94], [89, 84], [87, 70], [86, 61], [85,...\n",
       "Name: v0, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor[\"v0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cor.iloc[0, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6170c7e636c2dbd0f88cb8f557866518ac6c7d83ee4f2709c7df3161954bfcc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
